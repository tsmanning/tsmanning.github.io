---
title: "Projects"
layout: splash
permalink: /projects/
header:
  overlay_color: "#000"
  overlay_filter: "0.15"
  overlay_image: /assets/images/yosemiteValley2015.jpg
excerpt: #""
intro:
  - excerpt: #""
---

<head>
<meta name="viewport" content="width=device-width, initial-scale=1">
<!link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
<style>
* {
  box-sizing: border-box;
}

/* Create two columns that float next to each other */
.column {
  float: left;
  padding: 30px;
}

.left{
  width: 40%;
}

.right{
  width: 60%;
}

/* Clear floats after the columns */
.row:after {
  content: "";
  display: table;
  clear: both;
}

/* Responsive layout - makes the two columns stack on top of each other instead of next to each other */
@media screen and (max-width: 800px) {
  .left, .right {
    width: 100%;
    padding: 10px;
  }
}

/* Restrict responsive image to a maximum size, using max-width property */
.responsive {
  width: 100%;
  max-width: 600px;
  height: auto;
}

h2 {
  <!-- margin-top: 0em; -->
  margin-bottom: 0em;
}

h3 {
  margin-top: 0em;
  margin-bottom: 0em;
}

ol {
  margin-top: 0em;
}

ul {
  margin-top: 0em;
}

</style>
</head>

<body>

<!-- Project 1 container -->
<h2 id="Unconscious inference in motion perception">Unconscious inference in motion perception</h2>
<div class="row">
  <div class="column left">
    <img src="/assets/images/UncertProj.png" alt="Displays in experiment are situated half a meter and
    one meter from the participant. Under our model of how humans make estimate object motion out in the world, this
    should make speed measurments on the far screen less reliable even if the stimuli are the same retinal size."
    width="600" class="responsive">
  </div>
  <div class="column right">
    <p>People show systematic biases in perceived speeds and directions of motion under many conditions when
       objects are hard to see<span>&#8212;</span>for example, under low contrast conditions like driving in the fog
       or rain. Under these low contrast conditions, object speeds appear to move slower than they actually are in
       the world. This phenomenon has most commonly been explained as an example of unconscious perceptual inference:
       the brain is constantly building a model of object speeds in the world that is a combination of current speed
       information coming in from the eyes (weighted by uncertainty in that measurement) and expectations about
       speeds (based on previous experience of how often speeds are encountered in the world).</p>

    <p>Another major component of these perceptual speed biases lies in the transformation of retinal speeds into
       world speeds. An ideal observer should perform speed inference in world-centered coordinates, which requires
       a transformation of the uncertainty in the retinal measurements into uncertainty in world speeds. The degree to
       which this uncertainty is scaled depends on distance and retinal eccentricity, which predicts that contrast-dependent
       speed biases should increase with viewing distance.</p>
  </div>
</div>

<div>
  <h3>Published reports</h3>
  <ol id="pubs1">
    <li>Manning TS, Pillow JW, Rokers B, Cooper EA. (2022) Humans make non-ideal inferences about world motion.
    Journal of Vision 2022;22(14):4054. doi: <a href="https://doi.org/10.1167/jov.22.14.4054">10.1167/jov.22.14.4054</a>.</li>
    <li>Manning TS, Naecker BN, McLean IR, Rokers B, Pillow JW, Cooper EA. (2022) A general framework for inferring Bayesian ideal observer models from psychophysical data. eNeuro. doi: <a href="https://doi.org/10.1523/ENEURO.0144-22.2022">10.1523/ENEURO.0144-22.2022</a>.</li>
    <li>Manning TS, Pillow JW, Rokers B, Cooper EA. (2023) Contrast-dependent speed biases are distance-dependent too. In Prep.</li>
  </ol>
</div>

<div>
  <h3>Code repositories</h3>
  <ul>
    <li>Stimulus <a href="https://github.com/tsmanning/priorCoords_ptbCode">[Code]</a></li>
    <li>Analysis <a href="https://github.com/tsmanning/priorCoords_analysis">[Code]</a></li>
    <li>Bayesian ideal observer framework <a href="https://github.com/tsmanning/bayesIdealObserverMoG">[Code]</a></li>
  </ul>
</div>

<!-- Project 2 container -->
<h2 id="Optimal neural codes for binocular disparity">Optimal neural codes for binocular disparity</h2>
<div class="row">
  <div class="column left">
    <img src="/assets/images/DisparityProj.png" alt="Top: a map of binocular disparities in a natural image.
    Middle: disparity histograms within the central 10 degrees of vision during navigation and food preparation tasks. Bottom:
    Fisher information in three populations of neurons in the brain that encode disparity." width="600" class="responsive">
  </div>
  <div class="column right">
    <p>Binocular disparity<span>&#8212;</span>the difference between where the same point in space falls
       on the two eyes<span>&#8212;</span>is a useful visual cue for creating a vivid percept of the world in 3D.
       How does the brain encode this cue though to maximize its usefulness for vision? In this collaborative project
       with Dr. Emma Alexander, Dr. Gregory DeAngelis, and Dr. Xin Huang we use a combination of image statistics and
       information theoretic analysis of neuronal responses to disparity in different brain areas to answer this question.</p>

    <p>In short, there is a balancing act in the brain between: 1) the precision with which neurons in the brain can represent a
       physical variable, 2) the amount of metabolic resources a neural population requires, and 3) the loss function used to shape
       the population's selectivity (e.g. should the brain maximize sensory signal fidelity or discriminability?). We believe
       that there is a fundamental change in the loss functions guiding sensory encoding as one ascends the roughly hierarchical
       network of the primate visual system.</p>
  </div>
</div>

<div>
  <h3>Published reports</h3>
  <ol id="pubs2">
    <li>Manning TS, Alexander E, Cumming BG, DeAngelis GC, Huang X, Cooper EA. (2023) Transformations of sensory
  information in the brain reflect a changing definition of optimality. <a href="https://tsmanning.github.io/files/Manningetal2023_InfoTransforms.pdf">Preprint.</a></li>
    <li>Manning TS, Alexander E, DeAngelis GC, Huang X, Cooper EA. (2021) Role of MT Disparity Tuning
    Biases in Figure-Ground Segregation. <a href="https://www.abstractsonline.com/pp8/#!/10485/presentation/24967">Poster</a> presented at Society for Neuroscience, Chicago, IL.</li>
  </ol>
</div>

<div>
  <h3>Code repositories</h3>
  <ul>
    <li>Analysis <a href="https://github.com/tsmanning/DisparityInfoProject">[Code]</a></li>
  </ul>
</div>

<!-- Project 3 container -->
<h2 id="Extracting heading from retinal images during eye movements">Extracting heading from retinal images during eye movements</h2>
<div class="row">
  <div class="column left">
    <img src="/assets/images/HeadingProj.png" alt="Self-motion can be simulated with random dot motion that forms an optic flow pattern.
    When you rotate your eyes to pursue a moving object, that distorts the flow pattern (making it consistent with a curved trajectory),
    but the brain somehow undoes these distortions and we percieve our correct heading." width="600" class="responsive">
  </div>
  <div class="column right">
    <p>The primate visual system has evolved to be highly sensitive to optic flow cues<span>&#8212;</span>complex patterns of motion
       that arise from an observer moving through the world<span>&#8212;</span>to create a vivid percept of self-motion. At the same time,
       we have also have highly motile eyes that constantly select new aspects of the visual scene on which to focus the high-resolution
       fovea. These eye movements produce additional motion on the retina that distorts the optic flow patterns arising from locomotion,
       potentially giving the visual system incorrect information about heading. However, we manage just fine navigating the world, the brain
       must have a way to extract heading information from these distorted images.</p>

    <p>This extraction process could use a forward model of the expected distortions from an impending eye movement (i.e. based on a extraretinal
       mechanism) or may simply extract heading-related components of the moving image based on differences in motion patterns caused by eye
       movements and locomotion (i.e. based on a retinal mechanism). We investigated which of these two theories was best supported by recording
       from heading-sensitive neurons in the macaque monkey brain while the monkeys tracked targets with their eyes that moved in front of an
       optic flow stimulus.</p>
  </div>
</div>

<div>
  <h3>Published reports</h3>
  <ol id="pubs3">
    <li>Manning TS, Britten KH. (2019) Retinal stabilization reveals limited influence of extraretinal signals on
  heading tuning in the medial superior temporal area. J Neurosci. 39 (41) 8064-8078. doi: <a href="https://doi.org/10.1523/JNEUROSCI.0388-19.2019">10.1523/JNEUROSCI.0388-19.2019</a>.</li>
  </ol>
</div>

<div>
  <h3>Code repositories</h3>
  <ul>
    <li>Analysis <a href="https://github.com/tsmanning/EfferenceCopyMST">[Code]</a></li>
  </ul>
</div>

</body>
